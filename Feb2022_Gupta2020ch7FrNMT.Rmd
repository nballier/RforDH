---
title: An R Markdown document converted from "https://github.com/PacktPublishing/Deep-Learning-with-R-Cookbook/blob/master/Chapter07/Neural%20machine%20translation.ipynb"
output: html_document
---


rm(list=ls())
rm(list = ls(all.names = TRUE))
reticulate::repl_python()

# https://github.com/PacktPublishing/Deep-Learning-with-R-Cookbook/blob/master/Chapter07/Neural%20machine%20translation.ipynb

# https://www.packtpub.com/product/deep-learning-with-r-cookbook/9781789805673

# adapetd to use French / English data from Tatoaba http://www.manythings.org/anki/fra-eng.zip

# Adapted from the jupyter notebook to Rmd  
https://rmarkdown.rstudio.com/docs/reference/convert_ipynb.html
# with French-English data 


# Adapted from the jupyter notebook to Rmd 
# ipynb to Rmd ------
# https://rmarkdown.rstudio.com/docs/reference/convert_ipynb.html
# convert to R Markdown
nb_rmd <- rmarkdown:::convert_ipynb(nb_file)
xfun::file_string(nb_rmd)

#Arguments
input	<- '/Users/nballier/Desktop/Gupta2020ch7NMT.ipynb' # Path to the input ‘.ipynb’ file.

#nb_rmd <- rmarkdown:::convert_ipynb(input, output = xfun::with_ext(input, "Rmd"))
nb_rmd <- rmarkdown:::convert_ipynb(Gupta2020ch7NMT.ipynb)

xfun::file_string(nb_rmd) # show file content

?convert_ipynb

xfun::file_string(nb_file)  

nb_rmd <- rmarkdown:::convert_ipynb(Gupta2020ch7NMT.ipynb)
xfun::file_string(nb_rmd) # show file content


# standard questions to chechk your environment (dialogue with python can be tricky)
```{r}
print(.libPaths())
print(sessionInfo())
print(version)
sessionInfo()

```

#setwd("C:/Users/Utilisateur/Desktop/RforDH-Keras/Word-basedNMT")
#  source("/Volumes/Transcend3/RforDH-Keras/Word-basedNMT/scriptWord.R")




# In https://github.com/rstudio/keras/issues/298 I found a similar symptom, and there J.J. Allaire gave the fix, which also helped my case:
#    restarting your R session will clear the error


```{r}
# MAKE SURE YOU INSTALL "tensorflow 2.2.1" or earlier, do not use the current version 
install_tensorflow(version = "2.0.0")
#install_tensorflow(version = "2.2.1")
library(tensorflow)


# loading the required libraries
# install.packages("keras")
library(keras)
#install.packages("stringr")
library(stringr)
#install.packages("reshape2")
library(reshape2)
library(purrr)
library(ggplot2)
#install.packages("readr")
library(readr)
#install.packages("stringi"
library(stringi)
```

# to find your couple of languages  http://www.manythings.org/anki/
# Date of this file:
# 2020-06-28

# This data is from the sentences_detailed.csv file from tatoeba.org.
http://tatoeba.org/files/downloads/sentences_detailed.csv


# to load your own dataset
# get the text files online
temp <- tempfile()
download.file("https://www.manythings.org/anki/fra-eng.zip",temp)
data <- read.table(unz(temp, "fra.txt"))
unlink(temp)



# lines <- readLines("/MYPATH/fra.txt", n = 10000)

lines <- readLines("/Users/nballier/Desktop/MT/COMPUTING/R-keras/fra-eng/fra.txt", n = 10000)


# get around: use file.choose() 
lines <- readLines(file.choose())



# check what you got
head(lines,4)
class(lines)

```{r}
# loading the first 10000 phrases from the data;this will be our input data
# lines <- readLines("C:\\Users\\Utilisateur\\Desktop\\fra.txt", n = 10000)
lines <- readLines("/Users/nballier/Desktop/MT/COMPUTING/R-keras/fra-eng/fra.txt", n = 10000) # http://www.manythings.org/anki/
sentences <- str_split(lines, "\t")  # = loads the file with tabs as a sep
sentences[1:10]
```



```{r}
#  cleaning the input data 
data_cleaning <- function(sentence) {
  sentence <- gsub('[[:punct:] ]+',' ',sentence) # to replace strings of characters
  sentence <- gsub("[^[:alnum:]\\-\\.\\s]", " ", sentence)   #
  sentence <- stringi::stri_trans_general(sentence, "latin-ascii") # 
  sentence <- tolower(sentence) # to lowercase
#   sentence = paste0("<start> ", sentence, " <stop>")
  sentence
}


sentences <- map(sentences,data_cleaning)
```


```{r}
# capturing the maximum length of statements in English and French 
english_sentences <- list()  ## the output is going to be of the list type
french_sentences <- list()

for(i in 1:length(sentences)){
    current_sentence <- sentences[i]%>%unlist()%>%str_split('\t')
    english_sentences <- append(english_sentences,current_sentence[1])
    french_sentences <- append(french_sentences,current_sentence[2])  
}
```


```{r}
# converting the data into a dataframe
data <- do.call(rbind, Map(data.frame, "French"=french_sentences,"English"=english_sentences))
head(data,10) # inspect first 10 lines
```


```{r}
# checking the maximum number of words in all the sentences in French and English phrases
french_length <- max(sapply(strsplit(as.character(data[,"French"] ), " "), length))
print(paste0("Maximum length of a sentence in French data:",french_length))

eng_length <- max(sapply(strsplit(as.character(data[,"English"] ), " "), length))
print(paste0("Maximum length of a sentence in English data:",eng_length))
```


```{r}
# defining a function for tokenization
tokenization <- function(lines){
    tokenizer = text_tokenizer()
    tokenizer =  fit_text_tokenizer(tokenizer,lines)
    return(tokenizer)
}
```


# if the following code snippet does not run for tokenisation and tensorflow is required; installation takes a while
```{r}
# install.packages("tensorflow")
# Then, use the install_tensorflow() function to install TensorFlow. Note that on Windows you need a working installation of Anaconda.

#library(tensorflow)
#install_tensorflow()
```




```{r}
# preparing French tokenizer # takes some 30 s.
french_tokenizer <- tokenization(data[,"French"])
french_vocab_size <- length(french_tokenizer$word_index)  + 1

print(paste0('French Vocabulary Size:',french_vocab_size))
```


```{r}
# preparing English tokenizer
eng_tokenizer <- tokenization(data[,"English"])
eng_vocab_size <- length(eng_tokenizer$word_index) + 1

print(paste0('English Vocabulary Size:',eng_vocab_size))
```


```{r}
# function to encode and pad sequences
encode_pad_sequences <- function(tokenizer, length, lines){
    # Encoding text to integers
    seq = texts_to_sequences(tokenizer,lines)
    # Padding text to maximum length sentence
    seq = pad_sequences(seq, maxlen=length, padding='post')
    return(seq)
}
```


```{r}
# dividing the data into training and testing datasets 
train_data <- data[1:9000,]
test_data <- data[9001:10000,]
```


```{r}
# preprocessing the training and testing data # takes 15 s.
x_train <- encode_pad_sequences(french_tokenizer,french_length,train_data[,"French"])
y_train <- encode_pad_sequences(eng_tokenizer,eng_length,train_data[,"English"])
y_train <- to_categorical(y_train,num_classes = eng_vocab_size)

x_test <- encode_pad_sequences(french_tokenizer,french_length,test_data[,"French"])
y_test <- encode_pad_sequences(eng_tokenizer,eng_length,test_data[,"English"])
y_test <- to_categorical(y_test,num_classes = eng_vocab_size)
```


```{r}
# defining network parameters for model
in_vocab <- french_vocab_size
out_vocab <- eng_vocab_size
in_timesteps <- french_length
out_timesteps <- eng_length
units <- 512 # = number of numbers used to represent  words with numbers (length of the vector) 
epochs <- 70 # how many times will the NN see the data
batch_size = 200  # how many sentences at a time to be  learnt by the neural network
```


```{r}
# building model
model <- keras_model_sequential()
model %>%
    layer_embedding(in_vocab,units, input_length=in_timesteps, mask_zero=TRUE) %>%
    layer_lstm(units = units) %>%
    layer_repeat_vector(out_timesteps)%>%
    layer_lstm(units,return_sequences = TRUE)%>%
    time_distributed(layer_dense(units = out_vocab, activation='softmax'))

# summary of the model
summary(model)
```

## Note that ?compile does not list the options of the optimiser, which would be SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam, Ftrl
## there are other R packages to access keras with R , eg {KerasR}.


```{r}
# compiling the model
model %>% compile(optimizer = "adam",loss = 'categorical_crossentropy')
```


```{r}
# defining callbacks and checkpoints

model_name <- "model_nmt"

checkpoint_dir <- "checkpoints_nmt"
dir.create(checkpoint_dir)
filepath <- file.path(checkpoint_dir, paste0(model_name,"weights.{epoch:02d}-{val_loss:.2f}.hdf5",sep=""))

cp_callback <- list(callback_model_checkpoint(mode = "min",
 filepath = filepath,
 save_best_only = TRUE,
 verbose = 1))
#  callback_early_stopping(patience = 100))
```

# here for 70 epochs 

```{r}
# training the model # adapted to measure the time it takes

system.time(
model %>% fit(x_train,y_train,epochs = epochs,batch_size = batch_size,validation_split = 0.2,callbacks = cp_callback,verbose = 2)
)
```

class(model)
summary(model)

```{python}
import tensorflow as tf
from tensorflow import py_get_attr_impl
py_get_attr_impl(x, name, silent)
```




system.time(
  model %>%
    fit(
      x_train,
      y_train,
      epochs = epochs,
      batch_size = batch_size,
      validation_split = 0.2,
      callbacks = cp_callback,
      verbose = 2)
)
# be patient !! 261 min, 4 hours 30 on my machine 
#    user    system   elapsed 
# 15670.315  1604.995  5303.841


# from here , this scripts departs from (Gupta, 2020) since `predict_classes()` is deprecated and and was removed #from tensorflow in version 2.6.


predicted <- model %>% predict(x_test)

output <- matrix(NA_real_, ncol = dim(predicted)[2], nrow = nrow(predicted))
for (j in seq_len(nrow(predicted)))
{
  output[j,] <- apply(predicted[j,,], 1, which.max) - 1L
}

eng_vocab <- names(eng_tokenizer$word_index)
french_vocab <- names(french_tokenizer$word_index)

index_to_word <- function(data_sample, word_index_dict){
   phrase = list()
   for(i in 1:length(data_sample)){
       index = data_sample[[i]]
       word = word_index_dict[index]
        word = if(!is.null(word)) word else "?"
       phrase = paste0(phrase," ",word)
   }
   return(phrase)
}


index_to_word(x_test[41,], french_vocab) # [1] " nous sommes medecins         "
index_to_word(y_test_raw[41,], eng_vocab) # [1] " we re doctors  "
index_to_word(output[41,], eng_vocab) #  [1] " we re normal  "

#######
new_sentence <- "Je vais"
x_new <- encode_pad_sequences(french_tokenizer,french_length,new_sentence)

predicted_new <- model %>% predict(x_new)
output_new <- apply(predicted_new[1,,], 1, which.max) - 1L
predicted_translation <- index_to_word(output_new, eng_vocab)
predicted_translation


nouv.phrase <-  "They are doctors."
x_nouv <- encode_pad_sequences(eng_tokenizer,eng_length,nouv.phrase)

predicted_nouv <- model %>% predict(x_nouv)
output_nouv <- apply(predicted_nouv[1,,], 1, which.max) - 1L
predicted_traduction <- index_to_word(output_nouv, french_vocab)
predicted_traduction



## save your model with all your environment
save.image("~/Desktop/Model.RData")


### ASSIGNMENT : 
# change some parameters  (number of epochs?, size of the training data)
# test sentences
# test optimisers

### FURTHER READING
# Exploring what is learnt in the early epochs:
# Zimina et al. 2020

# to learn more about loss (stop reading before the different ways to compute loss are explained)
this blog : https://github.com/christianversloot/machine-learning-articles/blob/main/about-loss-and-loss-functions.md


